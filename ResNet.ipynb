{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "\n",
    "train_dir = 'data/train_images'\n",
    "test_dir = 'data/test_images'\n",
    "train_csv = 'data/train.csv'\n",
    "test_csv = 'data/test.csv'\n",
    "output_csv = 'data/output.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables \n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 25\n",
    "learning_rate = 0.001\n",
    "num_features = 6\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_df = pd.read_csv(train_csv)\n",
    "test_df = pd.read_csv(test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Train and Test's ancillary information\n",
    "feature_columns = train_df.columns[1:-6]\n",
    "\n",
    "# Calculate mean and std for each feature column\n",
    "feature_means = train_df[feature_columns].mean()\n",
    "feature_stds = train_df[feature_columns].std()\n",
    "\n",
    "# Normalize the features\n",
    "train_df[feature_columns] = (train_df[feature_columns] - feature_means) / feature_stds\n",
    "test_df[feature_columns] = (test_df[feature_columns] - feature_means) / feature_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.03624107e+00 1.48317376e+02 1.97016450e+04 3.48191181e+03\n",
      " 1.51120666e+01 3.99120598e+05]\n",
      "[1.37329381e-01 6.91740145e+00 4.31037489e+00 6.70979751e+01\n",
      " 5.93192463e-01 2.25494269e+03]\n"
     ]
    }
   ],
   "source": [
    "# Normalize Y label\n",
    "output_features = train_df.iloc[:, -6:].values\n",
    "\n",
    "output_mean = np.mean(output_features, axis=0)\n",
    "output_std = np.std(output_features, axis=0)\n",
    "\n",
    "print(output_mean)\n",
    "print(output_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset, returns image, labels, and ID\n",
    "\n",
    "class PlantDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the image path from the image directory and DataFrame\n",
    "        img_name = os.path.join(self.image_dir, str(self.dataframe.iloc[idx, 0]) + '.jpeg')\n",
    "        \n",
    "        # Load the image\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        \n",
    "        # Extract the labels (last 6 columns) from the DataFrame\n",
    "        labels = self.dataframe.iloc[idx, -6:].values.astype('float')\n",
    "        \n",
    "        # Apply transformations, if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Return the image and its corresponding labels\n",
    "        return image, torch.tensor(labels), str(self.dataframe.iloc[idx, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\howew\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\howew\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Transformation\n",
    "# Transforms without normalizing first\n",
    "\n",
    "# unnormalized_transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "# train_unnormalized_dataset = PlantDataset(train_df, train_dir, transform=unnormalized_transform)\n",
    "# train_unnormalized_loader = DataLoader(train_unnormalized_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# test_unnormalize_dataset = PlantDataset(test_df, test_dir, transform=unnormalized_transform)\n",
    "# test_unnormalized_loader = DataLoader(test_df, batch_size=1, shuffle=False)\n",
    "\n",
    "# mean_sum = torch.zeros(3)\n",
    "# std_sum = torch.zeros(3)\n",
    "# n_images = 0\n",
    "\n",
    "# print(len(train_unnormalized_loader))\n",
    "\n",
    "# Calculate mean and std, commented out since only needs to calculate once\n",
    "\n",
    "# for images in tqdm(train_unnormalized_loader):\n",
    "#     image = images[0][0]\n",
    "\n",
    "#     R_channel = image[0]\n",
    "#     G_channel = image[1]\n",
    "#     B_channel = image[2]\n",
    "\n",
    "#     R_array = np.array(R_channel, dtype=np.float32)\n",
    "#     G_array = np.array(G_channel, dtype=np.float32)\n",
    "#     B_array = np.array(B_channel, dtype=np.float32)\n",
    "\n",
    "#     # Stack to form a 3D array (H, W, C)\n",
    "#     image_array = np.stack((R_array, G_array, B_array), axis=-1)\n",
    "\n",
    "#     # Update accumulators\n",
    "#     mean_sum += image_array.mean(axis=(0, 1))\n",
    "#     std_sum += image_array.std(axis=(0, 1))\n",
    "\n",
    "# for images in tqdm(test_unnormalized_loader):\n",
    "#     image = images[0][0]\n",
    "\n",
    "#     R_channel = image[0]\n",
    "#     G_channel = image[1]\n",
    "#     B_channel = image[2]\n",
    "\n",
    "#     R_array = np.array(R_channel, dtype=np.float32) / 255.0\n",
    "#     G_array = np.array(G_channel, dtype=np.float32) / 255.0\n",
    "#     B_array = np.array(B_channel, dtype=np.float32) / 255.0\n",
    "\n",
    "#     # Stack to form a 3D array (H, W, C)\n",
    "#     image_array = np.stack((R_array, G_array, B_array), axis=-1)\n",
    "\n",
    "#     # Update accumulators\n",
    "#     mean_sum += image_array.mean(axis=(0, 1))\n",
    "#     std_sum += image_array.std(axis=(0, 1))\n",
    "\n",
    "# mean_sum /= len(train_unnormalized_loader)\n",
    "# std_sum /= len(train_unnormalized_loader)\n",
    "\n",
    "# print(mean_sum)\n",
    "# print(std_sum)\n",
    "\n",
    "\n",
    "\n",
    "# Tranfroms Dataset with mean and std\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4467, 0.4507, 0.3363], std=[0.1756, 0.1715, 0.1699])\n",
    "])\n",
    "\n",
    "full_dataset = PlantDataset(train_df, train_dir, transform=transform)\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = PlantDataset(test_df, test_dir, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Train with Resnet50\n",
    "\n",
    "model = models.resnet50(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, num_features)\n",
    "model = model.to(device)\n",
    "\n",
    "# Using MSELoss and Adam\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model using training set with mock R2\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, _ in dataloader:\n",
    "            labels = (labels - output_mean) / output_std\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predictions.append(outputs.cpu().numpy())\n",
    "            true_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_labels = np.concatenate(true_labels, axis=0)\n",
    "    return r2_score(true_labels, predictions)\n",
    "\n",
    "# Save results to csv\n",
    "\n",
    "def predict_and_save(model, dataloader, device, output_file):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, id in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.cpu().numpy()\n",
    "            outputs = outputs * output_std + output_mean  \n",
    "                      \n",
    "            for id_, output in zip(id, outputs):\n",
    "                predictions.append([id_, *output])  # Combine ID with predicted features\n",
    "\n",
    "    # Convert predictions to a DataFrame\n",
    "    prediction_df = pd.DataFrame(predictions, columns=['id', 'X4', 'X11', 'X18' ,'X26', 'X50', 'X3112'])\n",
    "\n",
    "    # Save to CSV\n",
    "    prediction_df.to_csv(output_file, index=False)\n",
    "    print(f\"Predictions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 1220/1220 [03:05<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Estimate R2 is 0.11404338730214976\n",
      "Predictions saved to data/output.csv\n",
      "Epoch 0/14, Loss: 0.8913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|██████████| 1220/1220 [03:09<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Estimate R2 is 0.10234350194368765\n",
      "Epoch 1/14, Loss: 0.8847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|██████████| 1220/1220 [03:05<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Estimate R2 is -0.04922298019206104\n",
      "Epoch 2/14, Loss: 0.8764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|██████████| 1220/1220 [03:05<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Estimate R2 is 0.11487680455972843\n",
      "Predictions saved to data/output.csv\n",
      "Epoch 3/14, Loss: 0.8696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|██████████| 1220/1220 [03:04<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Estimate R2 is 0.10283938475519148\n",
      "Epoch 4/14, Loss: 0.8640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|██████████| 1220/1220 [03:02<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Estimate R2 is 0.12339798639367626\n",
      "Predictions saved to data/output.csv\n",
      "Epoch 5/14, Loss: 0.8553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|██████████| 1220/1220 [03:10<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Estimate R2 is 0.12052827802039728\n",
      "Epoch 6/14, Loss: 0.8453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|██████████| 1220/1220 [03:15<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Estimate R2 is 0.12275472525131954\n",
      "Epoch 7/14, Loss: 0.8317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|██████████| 1220/1220 [03:08<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Estimate R2 is 0.08379643566022522\n",
      "Epoch 8/14, Loss: 0.8170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|██████████| 1220/1220 [03:06<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Estimate R2 is 0.09289280137562789\n",
      "Epoch 9/14, Loss: 0.7978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|██████████| 1220/1220 [03:13<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Estimate R2 is 0.07652468423822358\n",
      "Epoch 10/14, Loss: 0.7680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|██████████| 1220/1220 [03:31<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Estimate R2 is 0.06364450042964752\n",
      "Epoch 11/14, Loss: 0.7357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|██████████| 1220/1220 [03:11<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Estimate R2 is 0.04430664819387481\n",
      "Epoch 12/14, Loss: 0.6920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|██████████| 1220/1220 [03:03<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Estimate R2 is -0.009229122996713432\n",
      "Epoch 13/14, Loss: 0.6472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15: 100%|██████████| 1220/1220 [03:04<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Estimate R2 is 0.015463282769075585\n",
      "Epoch 14/14, Loss: 0.6021\n"
     ]
    }
   ],
   "source": [
    "maxR2 = -100\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 15\n",
    "best_model= model\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Train model\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "\n",
    "        # Update\n",
    "        labels = (labels - output_mean) / output_std\n",
    "        inputs, labels = inputs.to(device).float(), labels.to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    currentR2 = evaluate_model(model, val_loader, device)\n",
    "    print(f\"Current Estimate R2 is {currentR2}\")\n",
    "    if (currentR2 > maxR2):\n",
    "        maxR2 = currentR2\n",
    "        best_model = model\n",
    "        predict_and_save(model, test_loader, device, output_csv)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch}/{num_epochs - 1}, Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1356/1356 [02:00<00:00, 11.29it/s]\n",
      "100%|██████████| 200/200 [00:20<00:00,  9.88it/s]\n"
     ]
    }
   ],
   "source": [
    "resnet_train_dataset = PlantDataset(train_df, train_dir, transform=transform)\n",
    "resnet_train_dataloader = DataLoader(resnet_train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "resnet_test_dataset = PlantDataset(test_df, test_dir, transform=transform)\n",
    "resnet_test_dataloader = DataLoader(resnet_test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "resnet_train_outputs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels, img_ids in tqdm(resnet_train_dataloader):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)  # Assume model outputs a feature vector\n",
    "        resnet_train_outputs.append(outputs.cpu().numpy())\n",
    "\n",
    "resnet_train_outputs = np.concatenate(resnet_train_outputs, axis=0)\n",
    "\n",
    "resnet_train_df = pd.DataFrame(resnet_train_outputs, columns=[f'ResNet_{i+1}' for i in range(resnet_train_outputs.shape[1])])\n",
    "\n",
    "new_train_df = pd.concat([train_df.iloc[:, 0], resnet_train_df, train_df.iloc[:, 1:]], axis=1)\n",
    "\n",
    "model.eval()\n",
    "resnet_test_outputs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels, img_ids in tqdm(resnet_test_dataloader):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)  # Assume model outputs a feature vector\n",
    "        resnet_test_outputs.append(outputs.cpu().numpy())\n",
    "\n",
    "resnet_test_outputs = np.concatenate(resnet_test_outputs, axis=0)\n",
    "\n",
    "resnet_test_df = pd.DataFrame(resnet_test_outputs, columns=[f'ResNet_{i+1}' for i in range(resnet_test_outputs.shape[1])])\n",
    "\n",
    "new_test_df = pd.concat([test_df.iloc[:, 0], resnet_test_df, test_df.iloc[:, 1:]], axis=1)\n",
    "\n",
    "new_train_df.to_csv('data/enhanced_train.csv', index=False)\n",
    "new_test_df.to_csv('data/enhanced_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id  ResNet_1  ResNet_2  ResNet_3  ResNet_4  ResNet_5  ResNet_6  \\\n",
      "0      101801795  0.041336  0.138568 -0.299940 -0.194096 -0.077347 -0.142380   \n",
      "1      115813315 -0.625196 -0.145219 -0.824124 -0.301115 -0.263433 -0.242254   \n",
      "2      173551949  0.080681 -0.285931 -0.130485 -0.262000  0.202304 -0.114180   \n",
      "3      148811120  0.089529  0.676764  0.946536  0.532315 -0.614573  1.715887   \n",
      "4      195108876  0.350251  0.206010  0.893665  0.153691 -0.321190  1.471846   \n",
      "...          ...       ...       ...       ...       ...       ...       ...   \n",
      "43358  172502909 -0.199004  0.663847 -0.050354 -0.218364 -0.638626  0.646785   \n",
      "43359  183294324  0.094806 -0.064742  1.128746  1.620218  0.048013  2.225168   \n",
      "43360  108577580 -0.292752 -0.073270 -0.489194 -0.256287  0.066474 -0.228707   \n",
      "43361  139067673 -0.218086  1.067038 -0.337774 -0.165488 -0.712103  0.300434   \n",
      "43362  195383621 -0.778621  0.939716 -0.456877 -0.284214 -0.872787 -0.224540   \n",
      "\n",
      "       WORLDCLIM_BIO1_annual_mean_temperature  \\\n",
      "0                                    0.958067   \n",
      "1                                    1.680651   \n",
      "2                                    1.734932   \n",
      "3                                    1.499101   \n",
      "4                                    1.452164   \n",
      "...                                       ...   \n",
      "43358                                1.498137   \n",
      "43359                                1.314039   \n",
      "43360                                1.181719   \n",
      "43361                                0.118391   \n",
      "43362                               -1.255244   \n",
      "\n",
      "       WORLDCLIM_BIO12_annual_precipitation  \\\n",
      "0                                 -0.342647   \n",
      "1                                  0.586666   \n",
      "2                                 -0.043940   \n",
      "3                                  1.658508   \n",
      "4                                  1.745091   \n",
      "...                                     ...   \n",
      "43358                              3.082214   \n",
      "43359                             -0.157254   \n",
      "43360                             -1.313961   \n",
      "43361                             -0.299623   \n",
      "43362                              0.113600   \n",
      "\n",
      "       WORLDCLIM_BIO13.BIO14_delta_precipitation_of_wettest_and_dryest_month  \\\n",
      "0                                              -0.174501                       \n",
      "1                                               1.513074                       \n",
      "2                                               0.888271                       \n",
      "3                                               1.512886                       \n",
      "4                                               1.168429                       \n",
      "...                                                  ...                       \n",
      "43358                                           1.696493                       \n",
      "43359                                           0.810966                       \n",
      "43360                                          -0.943201                       \n",
      "43361                                           0.232947                       \n",
      "43362                                          -0.505919                       \n",
      "\n",
      "       ...  VOD_X_1997_2018_multiyear_mean_m09  \\\n",
      "0      ...                            0.419146   \n",
      "1      ...                           -0.204424   \n",
      "2      ...                           -0.767512   \n",
      "3      ...                            1.313934   \n",
      "4      ...                            1.761679   \n",
      "...    ...                                 ...   \n",
      "43358  ...                            0.864578   \n",
      "43359  ...                            0.268289   \n",
      "43360  ...                           -1.380782   \n",
      "43361  ...                            0.188456   \n",
      "43362  ...                            1.269882   \n",
      "\n",
      "       VOD_X_1997_2018_multiyear_mean_m10  VOD_X_1997_2018_multiyear_mean_m11  \\\n",
      "0                                0.251687                            0.209813   \n",
      "1                               -0.120948                            0.104043   \n",
      "2                               -0.708697                           -0.315483   \n",
      "3                                1.345499                            1.398733   \n",
      "4                                1.808304                            2.089294   \n",
      "...                                   ...                                 ...   \n",
      "43358                            0.873609                            0.933143   \n",
      "43359                            0.240258                            0.313557   \n",
      "43360                           -1.468562                           -1.560561   \n",
      "43361                            0.293589                            0.135073   \n",
      "43362                            0.846704                            0.550720   \n",
      "\n",
      "       VOD_X_1997_2018_multiyear_mean_m12   X4_mean    X11_mean      X18_mean  \\\n",
      "0                                0.249487  1.035657  142.521015  19699.923668   \n",
      "1                                0.281317  0.980728  153.726248  19699.721088   \n",
      "2                                0.169430  1.373851  137.016532  19702.276217   \n",
      "3                                1.117927  0.790627  162.022021  19702.424188   \n",
      "4                                2.262578  1.004912  154.428170  19701.160757   \n",
      "...                                   ...       ...         ...           ...   \n",
      "43358                            0.918223  1.032267  147.941775  19699.332901   \n",
      "43359                            0.419095  1.257809  143.195229  19716.303835   \n",
      "43360                           -1.615867  0.897972  153.277343  19699.259529   \n",
      "43361                           -0.008859  0.933451  153.229181  19699.217682   \n",
      "43362                            0.374576  0.847038  149.359591  19699.534799   \n",
      "\n",
      "          X26_mean   X50_mean     X3112_mean  \n",
      "0      3465.054691  15.842202  399384.490146  \n",
      "1      3462.940457  14.456965  398961.220402  \n",
      "2      3459.473270  15.833161  397614.158049  \n",
      "3      3480.277051  14.684226  402414.611731  \n",
      "4      3487.689253  15.023368  404405.289639  \n",
      "...            ...        ...            ...  \n",
      "43358  3459.412566  14.273777  398262.005212  \n",
      "43359  3548.911496  15.143285  402365.630661  \n",
      "43360  3459.489300  14.899656  397843.108882  \n",
      "43361  3459.441127  14.830224  397663.256983  \n",
      "43362  3463.420460  14.790158  398223.667465  \n",
      "\n",
      "[43363 rows x 176 columns]\n"
     ]
    }
   ],
   "source": [
    "print(new_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
